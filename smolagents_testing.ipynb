{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4396461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae2b2c",
   "metadata": {},
   "source": [
    "## Simple conversation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4830783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from openai import OpenAI\n",
    "class Simple_Conversation_Tool(Tool):\n",
    "    \"\"\"\n",
    "    A tiny small-talk tool that delegates response crafting to OpenAI.\n",
    "    Great for greetings, pleasantries, and casual chit-chat.\n",
    "    \"\"\"\n",
    "    name = \"Simple_Conversation_Tool\"\n",
    "    description = (\n",
    "        \"Handles simple conversational messages like greetings and small talk. \"\n",
    "        \"Use this tool when the user says hi/hello/hey/good morning, asks how you are, \"\n",
    "        \"thanks you, or otherwise initiates casual chat.\"\n",
    "    )\n",
    "\n",
    "    inputs = {\n",
    "        \"message\": {\"type\": \"string\", \"description\": \"The user's chat message.\"}\n",
    "    }\n",
    "\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, api_key: str, model_id: str = \"gpt-4o-mini\"):\n",
    "        super().__init__()\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model_id = model_id\n",
    "        self.system_prompt = (\n",
    "            \"You are a friendly, concise customer support greeter. \"\n",
    "            \"Respond in one or two sentences maximum. \"\n",
    "            \"Be warm, helpful, and professional. \"\n",
    "            \"If the user asks for support beyond greetings, invite them to share details.\"\n",
    "        )\n",
    "\n",
    "    def forward(self, message: str) -> str:\n",
    "        # Lightweight guardrail: only handle small-talk; let the agent do other stuff\n",
    "        # smalltalk_triggers = (\n",
    "        #     \"hi\", \"hello\", \"hey\", \"good morning\", \"good afternoon\",\n",
    "        #     \"good evening\", \"how are you\", \"what's up\", \"sup\", \"salam\",\n",
    "        #     \"assalam\", \"thanks\", \"thank you\"\n",
    "        # )\n",
    "        msg_lower = message.lower()\n",
    "        # if not any(t in msg_lower for t in smalltalk_triggers):\n",
    "        #     return (\n",
    "        #         \"I'm here for quick greetings and small talk. \"\n",
    "        #         \"Tell me a bit more about what you need help with!\"\n",
    "        #     )\n",
    "\n",
    "        # Ask OpenAI to craft the short, friendly reply\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            temperature=0.6,\n",
    "            max_tokens=80,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7d370",
   "metadata": {},
   "source": [
    "## FAQ TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb82b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from docx import Document\n",
    "from smolagents import Tool, ToolCallingAgent\n",
    "from smolagents.models import OpenAIModel\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "class FAQTool(Tool):\n",
    "    name = \"FAQTool\"\n",
    "    description = (\n",
    "        \"Answer customer FAQ-style questions strictly using the local file \"\n",
    "        \"`general_faqs.docx` (same directory as this script). \"\n",
    "        \"Use for queries about shipping, returns, refunds, warranties, hours, policies, pricing, etc.\"\n",
    "    )\n",
    "\n",
    "    inputs = {\n",
    "        \"question\": {\"type\": \"string\", \"description\": \"User's FAQ-style question.\"}\n",
    "    }\n",
    "\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str,\n",
    "        model_id: str = \"gpt-4o-mini\",\n",
    "        docx_path: str | Path = \"general_faqs.docx\",\n",
    "        max_snippets: int = 8\n",
    "    ):\n",
    "        \"\"\"\n",
    "        - Loads the docx once and keeps paragraphs in memory.\n",
    "        - Selects top-k relevant paragraphs to stay within token limits.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model_id = model_id\n",
    "        self.docx_path = Path(docx_path)\n",
    "        self.max_snippets = max_snippets\n",
    "        self.paragraphs = self._load_docx_paragraphs(self.docx_path)\n",
    "\n",
    "        self.system_prompt = (\n",
    "            \"You are a customer support assistant. Answer ONLY using the provided DATA. \"\n",
    "            \"If the DATA does not contain the answer, say you don't have enough information. \"\n",
    "            \"Be concise and actionable.\"\n",
    "        )\n",
    "\n",
    "    # ---- File loading helpers ----\n",
    "    @staticmethod\n",
    "    def _load_docx_paragraphs(path: Path) -> List[str]:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"FAQ file not found: {path.resolve()}\")\n",
    "        doc = Document(str(path))\n",
    "        paras = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]\n",
    "        # Merge tiny lines into bigger blocks (optional – keeps context coherent)\n",
    "        merged: List[str] = []\n",
    "        buf = []\n",
    "        for p in paras:\n",
    "            buf.append(p)\n",
    "            if len(\" \".join(buf)) > 300:  # simple merge threshold\n",
    "                merged.append(\" \".join(buf))\n",
    "                buf = []\n",
    "        if buf:\n",
    "            merged.append(\" \".join(buf))\n",
    "        return merged\n",
    "\n",
    "    # ---- Relevance selector: pick paragraphs most similar to the question ----\n",
    "    @staticmethod\n",
    "    def _keywordize(text: str) -> List[str]:\n",
    "        tokens = re.findall(r\"[a-z0-9]+\", text.lower())\n",
    "        stop = {\n",
    "            \"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"is\",\"are\",\"be\",\"for\",\"on\",\"in\",\n",
    "            \"at\",\"by\",\"with\",\"from\",\"as\",\"that\",\"this\",\"it\",\"we\",\"you\",\"your\",\n",
    "            \"our\",\"us\",\"i\"\n",
    "        }\n",
    "        return [t for t in tokens if t not in stop]\n",
    "\n",
    "    def _score(self, paragraph: str, question: str) -> int:\n",
    "        q = set(self._keywordize(question))\n",
    "        p = set(self._keywordize(paragraph))\n",
    "        return len(q & p)\n",
    "\n",
    "    def _select_relevant_context(self, question: str) -> List[Tuple[int, str]]:\n",
    "        scored = [(self._score(p, question), p) for p in self.paragraphs]\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        top = [(s, p) for s, p in scored[: self.max_snippets] if s > 0]\n",
    "        # If nothing matched, still return a small sample to give the LLM something\n",
    "        if not top and self.paragraphs:\n",
    "            top = [(0, self.paragraphs[0])]\n",
    "        # Attach indices to keep order stable (optional)\n",
    "        return list(enumerate([p for _, p in top], start=1))\n",
    "\n",
    "    # ---- Tool entrypoint ----\n",
    "    def forward(self, question: str) -> str:\n",
    "        # Lightweight trigger guard (optional; the agent usually routes to this tool)\n",
    "        faq_cues = [\n",
    "            \"refund\", \"return\", \"shipping\", \"delivery\", \"warranty\", \"replacement\",\n",
    "            \"order status\", \"track\", \"payment\", \"billing\", \"price\", \"discount\",\n",
    "            \"hours\", \"open\", \"close\", \"support\", \"policy\", \"policies\", \"exchange\",\n",
    "            \"cancel\", \"cancellation\", \"international\", \"customs\", \"duty\", \"tax\"\n",
    "        ]\n",
    "        if not any(cue in question.lower() for cue in faq_cues):\n",
    "            # Still proceed; the agent decided to call us. But we warn if it looks off.\n",
    "            pass\n",
    "\n",
    "        context_snippets = self._select_relevant_context(question)\n",
    "        context_text = \"\\n\\n\".join(\n",
    "            f\"[Snippet {i}]\\n{para}\" for i, para in context_snippets\n",
    "        )\n",
    "\n",
    "        user_payload = (\n",
    "            f\"DATA:\\n{context_text}\\n\\n\"\n",
    "            f\"QUESTION:\\n{question}\\n\\n\"\n",
    "            \"INSTRUCTIONS:\\n\"\n",
    "            \"- Answer ONLY using DATA.\\n\"\n",
    "            \"- If not in DATA, say: 'I’m sorry, I don’t have that information.'\\n\"\n",
    "            \"- Keep it concise (2–5 sentences).\"\n",
    "        )\n",
    "\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_payload},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=300,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4c7ff",
   "metadata": {},
   "source": [
    "## Answer Verification Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3836d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerifyAnswerTool(Tool):\n",
    "    \"\"\"\n",
    "    Verifies if the user's question has been sufficiently answered\n",
    "    by the latest tool output. If yes, returns 'FINALIZE: <answer>'.\n",
    "    Otherwise returns 'CONTINUE: <reason>'.\n",
    "    \"\"\"\n",
    "    name = \"VerifyAnswerTool\"\n",
    "    description = (\n",
    "        \"Given the original user question and the latest candidate answer, \"\n",
    "        \"decide if the question has been fully answered. If sufficient, return \"\n",
    "        \"'FINALIZE: <answer>'. If not, return 'CONTINUE: <brief reason>'.\"\n",
    "    )\n",
    "\n",
    "    inputs = {\n",
    "        \"question\": {\"type\": \"string\", \"description\": \"Original user question.\"},\n",
    "        \"candidate_answer\": {\"type\": \"string\", \"description\": \"Latest tool/model output you're evaluating.\"}\n",
    "    }\n",
    "\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, api_key: str, model_id: str = \"gpt-4o-mini\"):\n",
    "        super().__init__()\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model_id = model_id\n",
    "        self.system_prompt = (\n",
    "            \"You are a strict verifier. Determine if the candidate answer is correct \"\n",
    "            \"answers the user's question. If YES, respond exactly as:\\n\"\n",
    "            \"FINALIZE: <Answer as is>\\n\"\n",
    "            \"If NO, respond exactly as:\\n\"\n",
    "            \"CONTINUE: <one-sentence reason what is missing or unclear>\\n\"\n",
    "            \"Do not add any other text.\"\n",
    "        )\n",
    "\n",
    "    def forward(self, question: str, candidate_answer: str) -> str:\n",
    "        msg = (\n",
    "            f\"QUESTION:\\n{question}\\n\\n\"\n",
    "            f\"CANDIDATE_ANSWER:\\n{candidate_answer}\\n\\n\"\n",
    "            \"Does CANDIDATE_ANSWER fully answer QUESTION?\"\n",
    "        )\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": msg},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=120,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3c22ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">what can you do for me</span>                                                                                          <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ OpenAIServerModel - gpt-4o-mini ───────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mwhat can you do for me\u001b[0m                                                                                          \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m OpenAIServerModel - gpt-4o-mini \u001b[0m\u001b[38;2;212;183;2m──────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'Simple_Conversation_Tool' with arguments: {'message': 'What can you do for me?'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'Simple_Conversation_Tool' with arguments: {'message': 'What can you do for me?'}                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: I'm here to help with any questions or support you may need! Please feel free to share more details \n",
       "about what you're looking for.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: I'm here to help with any questions or support you may need! Please feel free to share more details \n",
       "about what you're looking for.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.52 seconds| Input tokens: 1,431 | Output tokens: 23]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.52 seconds| Input tokens: 1,431 | Output tokens: 23]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"I'm here to help with any questions or support you may │\n",
       "│ need! Please feel free to share more details about what you're looking for.\"}                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"I'm here to help with any questions or support you may │\n",
       "│ need! Please feel free to share more details about what you're looking for.\"}                                   │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: I'm here to help with any questions or support you may need! Please feel free to share more details \n",
       "about what you're looking for.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: I'm here to help with any questions or support you may need! Please feel free to share more details \n",
       "about what you're looking for.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: I'm here to help with any questions or support you may need! Please feel free to share more details </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">about what you're looking for.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: I'm here to help with any questions or support you may need! Please feel free to share more details \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mabout what you're looking for.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 1.18 seconds| Input tokens: 2,958 | Output tokens: 62]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 1.18 seconds| Input tokens: 2,958 | Output tokens: 62]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: what can you do for me\n",
      "Agent: I'm here to help with any questions or support you may need! Please feel free to share more details about what you're looking for.\n"
     ]
    }
   ],
   "source": [
    "from smolagents import Tool, ToolCallingAgent\n",
    "from smolagents.models import OpenAIModel\n",
    "from smolagents import FinalAnswerTool, UserInputTool\n",
    "\n",
    "# --- Create the OpenAI model ---\n",
    "# You can also set OPENAI_API_KEY in your environment instead of passing api_key directly.\n",
    "model = OpenAIModel(\n",
    "    model_id=\"gpt-4o-mini\",             # or \"gpt-4o\", \"gpt-4.1\", etc.\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# --- Register tools ---\n",
    "tools = [\n",
    "        Simple_Conversation_Tool(api_key=OPENAI_API_KEY, model_id=\"gpt-4o-mini\"),\n",
    "        FAQTool(api_key=OPENAI_API_KEY, model_id=\"gpt-4o-mini\", docx_path=\"general_faqs.docx\"),\n",
    "        FinalAnswerTool(),\n",
    "        UserInputTool()\n",
    "    ]\n",
    "\n",
    "INSTRUCTIONS = (\n",
    "    \"You are a tool-using customer support agent.\\n\"\n",
    "    \"Protocol for EVERY user message:\\n\"\n",
    "    \"1) Select exactly ONE domain tool (e.g., Simple_Conversation_Tool for greetings, \"\n",
    "    \"   FAQTool for policy questions) and call it ONCE to produce a CANDIDATE_ANSWER.\\n\"\n",
    "    \"2) If the CANDIDATE_ANSWER fully addresses the user’s question:\\n\"\n",
    "    \"   - Call FinalAnswerTool with that answer and STOP.\\n\"\n",
    "    \"3) If the CANDIDATE_ANSWER is incomplete, unclear, or irrelevant:\\n\"\n",
    "    \"   - Call another domain tool that is most likely to improve the answer.\\n\"\n",
    "    \"   - OR, if clarification is absolutely required, call UserInputTool to ask the user.\\n\"\n",
    "    \"   - After refining, repeat step 2.\\n\"\n",
    "    \"4) If user question is beyond your capabilites answer, call UserInputTool \"\n",
    "    \"   to request input from your customer support employee.\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- NEVER invent extra user messages or simulate a conversation with yourself.\\n\"\n",
    "    \"- Keep tool calls minimal — finalize as soon as you have a solid answer.\\n\"\n",
    "    \"- Always prefer short, clear, helpful answers.\\n\"\n",
    ")\n",
    "\n",
    "# --- Create the agent ---\n",
    "agent = ToolCallingAgent(\n",
    "        model=model,\n",
    "        tools=tools,\n",
    "        instructions=INSTRUCTIONS,\n",
    "        max_steps=5  # domain tool -> verify -> maybe one more hop -> verify -> finalize\n",
    "    )\n",
    "\n",
    "# --- Run queries ---\n",
    "query = input(\"User: \")\n",
    "response = agent.run(query)\n",
    "\n",
    "print(\"User:\", query)\n",
    "print(\"Agent:\", response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
